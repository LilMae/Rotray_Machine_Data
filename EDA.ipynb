{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_root = os.path.join(os.getcwd(), 'dataset')\n",
    "new_data_root = os.path.join(os.getcwd(), 'new_dataset')\n",
    "if os.path.isdir(new_data_root):\n",
    "    shutil.rmtree(new_data_root)\n",
    "os.mkdir(new_data_root)\n",
    "\n",
    "header = ['motor_x', 'motor_y', 'disk_x', 'disk_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 저장 규칙\n",
    "1. {class_name}{rpm}{cnt}{severity}.csv 로 한다  \n",
    "`{class_name}_{sampling_rate_str}_{rpm}_{severity}_{load_condition}_{cnt}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FaultDXAI\n",
    "paper name : `Fault Diagnosis using eXplainable AI: A transfer learning-based approach for rotating machinery exploiting augmented synthetic data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Test 12_Looseness : 420 files exist\n",
      "for Test 20_Unbalance : 420 files exist\n",
      "for Test 06_Normal Condition : 420 files exist\n",
      "for Test 16_Misalignment : 420 files exist\n",
      "for Test 13_Misalignment : 420 files exist\n",
      "for Test 11_Misalignment : 420 files exist\n",
      "for Test 10_Misalignment : 420 files exist\n",
      "for Test 14_Looseness : 420 files exist\n",
      "for Test 07_Unbalance : 420 files exist\n",
      "for Test 18_Unbalance : 420 files exist\n",
      "for Test 02_Misalignment : 420 files exist\n",
      "for Test 17_Normal Condition : 420 files exist\n",
      "for Test 01_Normal Condition : 420 files exist\n",
      "for Test 15_Unbalance : 420 files exist\n",
      "for Test 03_Normal Condition : 420 files exist\n",
      "for Test 09_Normal Condition : 420 files exist\n",
      "for Test 05_Looseness : 420 files exist\n",
      "for Test 04_Unbalance : 420 files exist\n",
      "for Test 19_Looseness : 420 files exist\n",
      "for Test 08_Looseness : 420 files exist\n"
     ]
    }
   ],
   "source": [
    "dxai_root = os.path.join(data_root, '1_FaultDXAI')\n",
    "\n",
    "rpm = 1238\n",
    "sampling_rate_str = '25kHz'\n",
    "load_condition = 'unknwon'\n",
    "severity = 'none'\n",
    "\n",
    "new_dxai_root = os.path.join(new_data_root, 'dxai')\n",
    "if os.path.isdir(new_dxai_root):\n",
    "    shutil.rmtree(new_dxai_root)\n",
    "os.mkdir(new_dxai_root)\n",
    "\n",
    "for test_name in os.listdir(dxai_root):\n",
    "    test_folder = os.path.join(dxai_root, test_name, test_name)\n",
    "    \n",
    "    class_name = test_name.split('_')[-1].split(' ')[0].lower()\n",
    "    test_folder_dist = os.path.join(new_dxai_root, class_name)\n",
    "    if not os.path.exists(test_folder_dist):\n",
    "        os.mkdir(test_folder_dist)\n",
    "    \n",
    "    print(f'for {test_name} : {len(os.listdir(test_folder))} files exist')\n",
    "    for test_file in os.listdir(test_folder):\n",
    "        file_path = os.path.join(test_folder, test_file)\n",
    "        file_np = np.load(file_path)\n",
    "        motor_y = file_np[0]\n",
    "        motor_x = file_np[1]\n",
    "        disk_y = file_np[2]\n",
    "        disk_x = file_np[3]\n",
    "        \n",
    "        data_pd = pd.DataFrame({'motor_x' : motor_x, \n",
    "                                'motor_y' : motor_y, \n",
    "                                'disk_x' : disk_x, \n",
    "                                'disk_y' : disk_y})\n",
    "        data_pd.to_csv(os.path.join(test_folder_dist, f'{class_name}_{sampling_rate_str}_{rpm}_{severity}_{load_condition}_{len(os.listdir(test_folder_dist))+1}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IIS \n",
    "paper name : `Machine Learning-Based Unbalance Detection of a Rotating Shaft Using Vibration Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : unbalance with 60.7mm g\n",
      "number of speed : 3322, total : 3323 slice exist\n",
      "Processing : unbalance with 75.5mm g\n",
      "number of speed : 3364, total : 3365 slice exist\n",
      "Processing : unbalance with 75.5mm g\n",
      "number of speed : 12878, total : 12879 slice exist\n",
      "Processing : unbalance with 152.1mm g\n",
      "number of speed : 3370, total : 3371 slice exist\n",
      "Processing : unbalance with 45.9mm g\n",
      "number of speed : 3368, total : 3369 slice exist\n",
      "Processing : unbalance with 60.7mm g\n",
      "number of speed : 12888, total : 12889 slice exist\n",
      "Processing : normal with 0.0mm g\n",
      "number of speed : 3358, total : 3359 slice exist\n",
      "Processing : unbalance with 45.9mm g\n",
      "number of speed : 12888, total : 12889 slice exist\n",
      "Processing : unbalance with 152.1mm g\n",
      "number of speed : 12880, total : 12881 slice exist\n",
      "Processing : normal with 0.0mm g\n",
      "number of speed : 12894, total : 12895 slice exist\n"
     ]
    }
   ],
   "source": [
    "iis_root = os.path.join(data_root, '2_IIS')\n",
    "\n",
    "# Parameters\n",
    "sampling_rate = 4096  # 4096 samples per second\n",
    "samplint_rate_str = '4096Hz'\n",
    "window_size = 1.0  # 1 second\n",
    "stride = 0.5  # 0.5 second\n",
    "load_condition = 'unknwon'\n",
    "\n",
    "new_iss_root = os.path.join(new_data_root, 'iis')\n",
    "if os.path.isdir(new_iss_root):\n",
    "    shutil.rmtree(new_iss_root)\n",
    "os.mkdir(new_iss_root)\n",
    "\n",
    "for test_path in glob.glob(os.path.join(iis_root,'*.csv')):\n",
    "    \n",
    "    test_name = os.path.split(test_path)[-1]\n",
    "    if int(test_name[0]) == 0:\n",
    "        class_name = 'normal'\n",
    "        severity = 0\n",
    "    else:\n",
    "        class_name = 'unbalance'\n",
    "        severity = [0, 459, 607, 755, 1521]\n",
    "        severity = severity[int(test_name[0])]\n",
    "    \n",
    "    test_folder_dist = os.path.join(new_iss_root, class_name)\n",
    "    if not os.path.exists(test_folder_dist):\n",
    "        os.mkdir(test_folder_dist)\n",
    "    \n",
    "    print(f'Processing : {class_name} with {severity/10}mm g')\n",
    "    \n",
    "    start = 0\n",
    "    data_sec = 0.0\n",
    "    cnt = 0\n",
    "    is_first_segment = True  # Flag to skip the first segment\n",
    "    \n",
    "    exp_pd = pd.read_csv(test_path)\n",
    "    \n",
    "    speed_df = exp_pd['Measured_RPM']\n",
    "    \n",
    "    change_indices = [0]  # Start with the first index\n",
    "    change_indices += speed_df.index[\n",
    "        (speed_df.diff().abs() > 10)\n",
    "    ].tolist()\n",
    "    \n",
    "    for change_index in change_indices[1:]:\n",
    "        end = change_index\n",
    "        \n",
    "        static_speed = exp_pd.iloc()[start:end]\n",
    "        \n",
    "        disk_x = static_speed['Vibration_1']\n",
    "        disk_y = static_speed['Vibration_2']\n",
    "        motor_y = static_speed['Vibration_3']\n",
    "        \n",
    "        data_pd = pd.DataFrame({'motor_x' : None, \n",
    "                                'motor_y' : motor_y, \n",
    "                                'disk_x' : disk_x, \n",
    "                                'disk_y' : disk_y})\n",
    "        \n",
    "        mean_rpm = int(static_speed['Measured_RPM'].mean())\n",
    "        \n",
    "        cnt += 1\n",
    "        start = end\n",
    "        \n",
    "        # Skip the first segment\n",
    "        if is_first_segment:\n",
    "            is_first_segment = False\n",
    "            continue\n",
    "        \n",
    "        # Slice data_pd by 1-second window with 0.5-second stride\n",
    "        window_samples = int(window_size * sampling_rate)\n",
    "        stride_samples = int(stride * sampling_rate)\n",
    "        num_rows = len(data_pd)\n",
    "        \n",
    "        for slice_start in range(0, num_rows - window_samples + 1, stride_samples):\n",
    "            slice_end = slice_start + window_samples\n",
    "            sliced_data = data_pd.iloc[slice_start:slice_end]\n",
    "            \n",
    "            cnt += 1\n",
    "            \n",
    "            # Save sliced data to a CSV file\n",
    "            output_file = os.path.join(test_folder_dist, f'{class_name}_{sampling_rate_str}_{mean_rpm}_{severity/10}mmg_{load_condition}_{cnt}.csv')\n",
    "            sliced_data.to_csv(output_file, index=False)\n",
    "        \n",
    "\n",
    "    print(f'number of speed : {cnt-1}, total : {cnt} slice exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. VAT-MCD\n",
    "paper name : `Vibration, acoustic, temperature, and motor current dataset of rotating machine under varying operating conditions for fault diagnosis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_condition : 2Nm, class_name : misalign, severity : 03\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : normal, severity : normal\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : misalign, severity : 03\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : bpfi, severity : 10\n",
      "num data slices : 119\n",
      "load_condition : 4Nm, class_name : misalign, severity : 03\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : unbalance, severity : 0583mg\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : misalign, severity : 01\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : bpfo, severity : 30\n",
      "num data slices : 119\n",
      "load_condition : 2Nm, class_name : bpfi, severity : 10\n",
      "num data slices : 119\n",
      "load_condition : 0Nm, class_name : unbalance, severity : 1169mg\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : misalign, severity : 05\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : normal, severity : normal\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : bpfi, severity : 03\n",
      "num data slices : 119\n",
      "load_condition : 2Nm, class_name : unbalance, severity : 1751mg\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : misalign, severity : 05\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : unbalance, severity : 2239mg\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : unbalance, severity : 2239mg\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : bpfo, severity : 03\n",
      "num data slices : 119\n",
      "load_condition : 2Nm, class_name : bpfi, severity : 30\n",
      "num data slices : 119\n",
      "load_condition : 0Nm, class_name : unbalance, severity : 3318mg\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : bpfi, severity : 03\n",
      "num data slices : 119\n",
      "load_condition : 4Nm, class_name : bpfo, severity : 10\n",
      "num data slices : 119\n",
      "load_condition : 0Nm, class_name : misalign, severity : 01\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : unbalance, severity : 3318mg\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : normal, severity : normal\n",
      "num data slices : 599\n",
      "load_condition : 4Nm, class_name : bpfi, severity : 10\n",
      "num data slices : 119\n",
      "load_condition : 0Nm, class_name : bpfo, severity : 10\n",
      "num data slices : 119\n",
      "load_condition : 0Nm, class_name : bpfo, severity : 03\n",
      "num data slices : 119\n",
      "load_condition : 2Nm, class_name : unbalance, severity : 2239mg\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : unbalance, severity : 3318mg\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : misalign, severity : 01\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : bpfi, severity : 30\n",
      "num data slices : 119\n",
      "load_condition : 4Nm, class_name : misalign, severity : 05\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : bpfi, severity : 03\n",
      "num data slices : 119\n",
      "load_condition : 2Nm, class_name : unbalance, severity : 1169mg\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : unbalance, severity : 1751mg\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : unbalance, severity : 0583mg\n",
      "num data slices : 239\n",
      "load_condition : 4Nm, class_name : unbalance, severity : 1169mg\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : bpfo, severity : 30\n",
      "num data slices : 119\n",
      "load_condition : 4Nm, class_name : unbalance, severity : 1751mg\n",
      "num data slices : 239\n",
      "load_condition : 0Nm, class_name : bpfi, severity : 30\n",
      "num data slices : 119\n",
      "load_condition : 0Nm, class_name : unbalance, severity : 0583mg\n",
      "num data slices : 239\n",
      "load_condition : 2Nm, class_name : bpfo, severity : 10\n",
      "num data slices : 119\n",
      "load_condition : 4Nm, class_name : bpfo, severity : 03\n",
      "num data slices : 119\n",
      "load_condition : 2Nm, class_name : bpfo, severity : 30\n",
      "num data slices : 119\n"
     ]
    }
   ],
   "source": [
    "vat_root = os.path.join(data_root, '3_VAT_mat')\n",
    "\n",
    "sampling_rate = 25600 # 25.6kHz\n",
    "sampling_rate_str = '25.6kHz'\n",
    "window_size = 1.0  # 1 second\n",
    "stride = 0.5  # 0.5 second\n",
    "rpm = 3010\n",
    "\n",
    "new_vat_root = os.path.join(new_data_root, 'vat')\n",
    "if os.path.isdir(new_vat_root):\n",
    "    shutil.rmtree(new_vat_root)\n",
    "os.mkdir(new_vat_root)\n",
    "\n",
    "for file_name in os.listdir(vat_root):\n",
    "    \n",
    "    load_condition = file_name.split('.')[0].split('_')[0]\n",
    "    class_name = file_name.split('.')[0].split('_')[1].lower()\n",
    "    if class_name != 'normal':\n",
    "        severity = file_name.split('.')[0].split('_')[2]\n",
    "    else:\n",
    "        severity = 'normal'\n",
    "    if class_name == 'unbalalnce':\n",
    "        class_name = 'unbalance'\n",
    "    \n",
    "    print(f'load_condition : {load_condition}, class_name : {class_name}, severity : {severity}')\n",
    "    \n",
    "    test_folder_dist = os.path.join(new_vat_root, class_name)\n",
    "    if not os.path.exists(test_folder_dist):\n",
    "        os.mkdir(test_folder_dist)\n",
    "    \n",
    "    file_path = os.path.join(vat_root, file_name)\n",
    "    \n",
    "    mat_file = io.loadmat(file_path)\n",
    "    signal = mat_file['Signal'][0][0][1][0][0][0]\n",
    "    data_pd = pd.DataFrame(signal, columns=['motor_x','motor_y','disk_x','disk_y'])\n",
    "    \n",
    "    # Slice data_pd by 1-second window with 0.5-second stride\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    stride_samples = int(stride * sampling_rate)\n",
    "    num_rows = len(data_pd)\n",
    "    \n",
    "    cnt = 0\n",
    "    for slice_start in range(0, num_rows - window_samples + 1, stride_samples):\n",
    "        slice_end = slice_start + window_samples\n",
    "        sliced_data = data_pd.iloc[slice_start:slice_end]\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "        # Save sliced data to a CSV file\n",
    "        output_file = os.path.join(test_folder_dist, f'{class_name}_{sampling_rate_str}_{rpm}_{severity}_{load_condition}_{cnt}.csv')\n",
    "        sliced_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f'num data slices : {cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. VBL-VA001\n",
    "\n",
    "paper_name : `Lab-Scale Vibration Analysis Dataset and Baseline Methods for Machinery Fault Diagnosis with Machine Learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:48<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data slices : 8000\n",
      "Processing : misalignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:52<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data slices : 8000\n",
      "Processing : bearing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:53<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data slices : 8000\n",
      "Processing : unbalance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:52<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data slices : 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vbl_root = os.path.join(data_root, '4_VBL-VA001')\n",
    "\n",
    "sampling_rate = 20000 # 20kHz\n",
    "sampling_rate_str = '20kHz'\n",
    "window_size = 1.0  # 1 second\n",
    "stride = 0.5  # 0.5 second\n",
    "rpm = 3000\n",
    "load_condition = 'unknown'\n",
    "\n",
    "new_vbl_root = os.path.join(new_data_root, 'vbl')\n",
    "if os.path.isdir(new_vbl_root):\n",
    "    shutil.rmtree(new_vbl_root)\n",
    "os.mkdir(new_vbl_root)\n",
    "\n",
    "\n",
    "for class_name in os.listdir(vbl_root):\n",
    "    class_folder = os.path.join(vbl_root, class_name)\n",
    "    \n",
    "    test_folder_dist = os.path.join(new_vbl_root, class_name)\n",
    "    if not os.path.exists(test_folder_dist):\n",
    "        os.mkdir(test_folder_dist)\n",
    "    \n",
    "    print(f'Processing : {class_name}')\n",
    "    cnt = 0\n",
    "    for file_name in tqdm(os.listdir(class_folder)):\n",
    "        file_path = os.path.join(class_folder, file_name)\n",
    "        data_pd = pd.read_csv(file_path, header=None)\n",
    "        data_pd.columns = ['time', 'motor_x', 'motor_y', 'motor_z']\n",
    "        data_pd = data_pd.drop(labels='time', axis=1)\n",
    "        \n",
    "        if class_name == 'unbalance':\n",
    "            severity = file_name.split('_')[1]\n",
    "            \n",
    "            if severity == 'z':\n",
    "                severity = file_name.split('_')[0][-2:]\n",
    "        else:\n",
    "            severity = 'none'\n",
    "        \n",
    "        # Slice data_pd by 1-second window with 0.5-second stride\n",
    "        window_samples = int(window_size * sampling_rate)\n",
    "        stride_samples = int(stride * sampling_rate)\n",
    "        num_rows = len(data_pd)\n",
    "        \n",
    "        \n",
    "        for slice_start in range(0, num_rows - window_samples + 1, stride_samples):\n",
    "            slice_end = slice_start + window_samples\n",
    "            sliced_data = data_pd.iloc[slice_start:slice_end]\n",
    "            \n",
    "            cnt += 1\n",
    "            \n",
    "            # Save sliced data to a CSV file\n",
    "            output_file = os.path.join(test_folder_dist, f'{class_name}_{sampling_rate_str}_{rpm}_{severity}_{load_condition}_{cnt}.csv')\n",
    "            sliced_data.to_csv(output_file, index=False)\n",
    "        \n",
    "    print(f'num data slices : {cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MaFaulDa\n",
    "paper name : `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfd_file_read_save(data_pd, folder_dist , class_name, severity):\n",
    "    sampling_rate_str = '50kHz'\n",
    "    \n",
    "    load_condition = 'unknown'\n",
    "    # Hyper Params\n",
    "    sampling_rate = 50000\n",
    "    mfd_columns = ['tachometer', 'motor_z', 'motor_y', 'motor_x', 'disk_z', 'disk_y', 'disk_x', 'microphone']\n",
    "    window_size = 1.0  # 1 second\n",
    "    stride = 0.5  # 0.5 second\n",
    "    \n",
    "    if not os.path.exists(folder_dist):\n",
    "        os.mkdir(folder_dist)\n",
    "    \n",
    "    data_pd.columns = mfd_columns\n",
    "\n",
    "    tachometer_np = data_pd['tachometer'].to_numpy()\n",
    "    tachometer_np = np.convolve(tachometer_np, np.ones(5) / 5, mode='same')\n",
    "    binary_np = (tachometer_np >3).astype(int)\n",
    "    rising_edges = np.where(np.diff(binary_np) > 0)[0]\n",
    "    pulse_intervals = np.diff(rising_edges) / sampling_rate \n",
    "\n",
    "    avg_time_per_revolution = np.mean(pulse_intervals)\n",
    "\n",
    "    rpm = int((1 / avg_time_per_revolution) * 60)\n",
    "\n",
    "\n",
    "    # Slice data_pd by 1-second window with 0.5-second stride\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    stride_samples = int(stride * sampling_rate)\n",
    "    num_rows = len(data_pd)\n",
    "\n",
    "    for slice_start in range(0, num_rows - window_samples + 1, stride_samples):\n",
    "        slice_end = slice_start + window_samples\n",
    "        sliced_data = data_pd.iloc[slice_start:slice_end]\n",
    "        \n",
    "        \n",
    "        # Save sliced data to a CSV file\n",
    "        output_file = os.path.join(folder_dist, f'{class_name}_{sampling_rate_str}_{rpm}_{severity}_{load_condition}_{len(os.listdir(folder_dist))+1}.csv')\n",
    "        sliced_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : horizontal-misalignment\n",
      "Processing : horizontal-misalignment/1.0mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:08<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : horizontal-misalignment/2.0mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:08<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : horizontal-misalignment/0.5mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:09<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : horizontal-misalignment/1.5mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:08<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : vertical-misalignment\n",
      "Processing : vertical-misalignment/0.63mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:10<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : vertical-misalignment/1.78mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:10<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : vertical-misalignment/0.51mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:11<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : vertical-misalignment/1.27mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:10<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : vertical-misalignment/1.90mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:10<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : vertical-misalignment/1.40mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:10<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : imbalance\n",
      "Processing : imbalance/10g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:07<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : imbalance/20g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:08<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : imbalance/25g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:06<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : imbalance/6g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:09<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : imbalance/15g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:07<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : imbalance/35g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [01:02<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : imbalance/30g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [01:05<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : normal\n",
      "Processing : normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:09<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang\n",
      "Processing : underhang/ball_fault/20g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:07<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/ball_fault/6g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:07<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/ball_fault/0g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:09<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/ball_fault/35g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:52<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/cage_fault/20g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:08<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/cage_fault/6g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:08<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/cage_fault/0g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:09<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/cage_fault/35g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:58<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/outer_race/20g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:08<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/outer_race/6g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:09<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/outer_race/0g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:09<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : underhang/outer_race/35g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:51<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : overhang\n",
      "Processing : overhang/ball_fault/20g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:33<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : overhang/ball_fault/6g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:58<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : overhang/ball_fault/0g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:06<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : overhang/ball_fault/35g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:27<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : overhang/cage_fault/20g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:07<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : overhang/cage_fault/6g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 36/49 [00:51<00:18,  1.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m                 dis_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(new_mfd_root, class_name)\n\u001b[1;32m     45\u001b[0m                 specific_class_name \u001b[38;5;241m=\u001b[39m class_name \u001b[38;5;241m+\u001b[39m specific_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 46\u001b[0m                 \u001b[43mmfd_file_read_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdis_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecific_class_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseverity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mmfd_file_read_save\u001b[0;34m(data_pd, folder_dist, class_name, severity)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Save sliced data to a CSV file\u001b[39;00m\n\u001b[1;32m     38\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_dist, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampling_rate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrpm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseverity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_condition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(folder_dist))\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43msliced_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/io/formats/csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/io/formats/csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/io/formats/csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/io/formats/csvs.py:320\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    317\u001b[0m slicer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(start_i, end_i)\n\u001b[1;32m    318\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[slicer]\n\u001b[0;32m--> 320\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_values_for_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_number_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[1;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/core/frame.py:1410\u001b[0m, in \u001b[0;36mDataFrame._get_values_for_csv\u001b[0;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_values_for_csv\u001b[39m(\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# helper used by to_csv\u001b[39;00m\n\u001b[0;32m-> 1410\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_values_for_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"DataFrame\", expected \"Self\")\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/core/internals/managers.py:466\u001b[0m, in \u001b[0;36mBaseBlockManager.get_values_for_csv\u001b[0;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_values_for_csv\u001b[39m(\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, float_format, date_format, decimal, na_rep: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, quoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    461\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    Convert values to native types (strings / python objects) that are used\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    in formatting (repr / csv).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget_values_for_csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/core/internals/blocks.py:780\u001b[0m, in \u001b[0;36mBlock.get_values_for_csv\u001b[0;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_values_for_csv\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, float_format, date_format, decimal, na_rep: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, quoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Block:\n\u001b[1;32m    779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"convert to our native types format\"\"\"\u001b[39;00m\n\u001b[0;32m--> 780\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mget_values_for_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pandas/core/indexes/base.py:7834\u001b[0m, in \u001b[0;36mget_values_for_csv\u001b[0;34m(values, date_format, na_rep, quoting, float_format, decimal)\u001b[0m\n\u001b[1;32m   7831\u001b[0m mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m   7833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quoting:\n\u001b[0;32m-> 7834\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7835\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7836\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mfd_root = os.path.join(data_root, '6_MaFaulDa')\n",
    "\n",
    "sampling_rate = 50000\n",
    "window_size = 1.0\n",
    "stride = 0.5\n",
    "\n",
    "new_mfd_root = os.path.join(new_data_root, 'mfd')\n",
    "if os.path.isdir(new_mfd_root):\n",
    "    shutil.rmtree(new_mfd_root)\n",
    "os.mkdir(new_mfd_root)\n",
    "\n",
    "for class_name in os.listdir(mfd_root):\n",
    "    \n",
    "    class_dir = os.path.join(mfd_root, class_name)\n",
    "    print(f'Processing : {class_name}')\n",
    "    if class_name in ['horizontal-misalignment', 'imbalance', 'vertical-misalignment']:\n",
    "        for severity in os.listdir(class_dir):\n",
    "            severity_dir = os.path.join(class_dir, severity)\n",
    "            print(f'Processing : {class_name}/{severity}')\n",
    "            for file_name in tqdm(os.listdir(severity_dir)):\n",
    "                \n",
    "                file_path = os.path.join(severity_dir, file_name)\n",
    "                \n",
    "                data_pd = pd.read_csv(file_path)\n",
    "                \n",
    "                dis_folder = os.path.join(new_mfd_root, class_name)\n",
    "\n",
    "                mfd_file_read_save(data_pd, dis_folder, class_name, severity)\n",
    "                \n",
    "            \n",
    "    elif class_name in ['overhang', 'underhang']:\n",
    "        for specific_class in os.listdir(class_dir):\n",
    "            specific_class_dir = os.path.join(class_dir, specific_class)\n",
    "            \n",
    "            for severity in os.listdir(specific_class_dir):\n",
    "                severity_dir = os.path.join(specific_class_dir, severity)\n",
    "                print(f'Processing : {class_name}/{specific_class}/{severity}')\n",
    "                \n",
    "                for file_name in tqdm(os.listdir(severity_dir)):\n",
    "                    file_path = os.path.join(severity_dir, file_name)\n",
    "                    data_pd = pd.read_csv(file_path)\n",
    "                    \n",
    "                    dis_folder = os.path.join(new_mfd_root, class_name)\n",
    "\n",
    "                    specific_class_name = class_name + specific_class.split('_')[0]\n",
    "                    mfd_file_read_save(data_pd, dis_folder, specific_class_name, severity)\n",
    "    \n",
    "    elif class_name == 'normal':\n",
    "        print(f'Processing : {class_name}')\n",
    "        for file_name in tqdm(os.listdir(class_dir)):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            data_pd = pd.read_csv(file_path)\n",
    "            \n",
    "            \n",
    "            dis_folder = os.path.join(new_mfd_root, class_name)\n",
    "\n",
    "            mfd_file_read_save(data_pd, dis_folder, class_name, 'none')\n",
    "            \n",
    "    else:\n",
    "        print(f'Wrong Class Name : {class_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snu_root = os.path.join(data_root, '5_SNU')\n",
    "\n",
    "window_size = 1.0\n",
    "stride = 0.5\n",
    "\n",
    "rotary_class_dict = {'H' : 'normal', \n",
    "                     'L' : 'losseness', \n",
    "                     'U1' : 'unbalance',    # 2g\n",
    "                     'U2' : 'unbalance',    # 3g\n",
    "                     'U3' : 'unbalance',    # 5g\n",
    "                     'M1' : 'misalignment', # 0.6mm\n",
    "                     'M2' : 'misalignment', # 0.8mm\n",
    "                     'M3' : 'misalignment'  # 1.0mm\n",
    "                     }\n",
    "bearing_class_dict = {'H' : 'normal', \n",
    "                      'B' : 'encompass', \n",
    "                      'IR': 'bpfi', \n",
    "                      'OR': 'bpfo'}\n",
    "unbalance_severity_list = ['2g', '3g', '5g']\n",
    "misalignment_severity_list = ['0.6mm', '0.8mm', '1mm']\n",
    "load_condition = 'unknown'\n",
    "\n",
    "new_snu_root = os.path.join(new_data_root, 'snu')\n",
    "if os.path.isdir(new_snu_root):\n",
    "    shutil.rmtree(new_snu_root)\n",
    "os.mkdir(new_snu_root)\n",
    "\n",
    "for bearing_type in os.listdir(snu_root):\n",
    "    bearing_dir = os.path.join(snu_root, bearing_type)\n",
    "    \n",
    "    for sampling_rate in os.listdir(bearing_dir):\n",
    "        sampling_rate_dir = os.path.join(bearing_dir, sampling_rate)\n",
    "        sampling_rate_str = f'{int(sampling_rate.split(\"_\")[-1])/1000}kHz'\n",
    "        sampling_rate = int(sampling_rate.split('_')[-1])\n",
    "        \n",
    "        for speed in os.listdir(sampling_rate_dir):\n",
    "            speed_dir = os.path.join(sampling_rate_dir, speed)\n",
    "            rpm = speed.split('_')[-1]\n",
    "            \n",
    "            print(f'Processing : {bearing_type}/{sampling_rate}/{speed}')\n",
    "            for file_name in tqdm(os.listdir(speed_dir)):\n",
    "                file_path = os.path.join(speed_dir, file_name)\n",
    "                mat_file = io.loadmat(file_path)\n",
    "                disk_y = mat_file['Data']\n",
    "                \n",
    "                rotary_class_query = file_name.split('_')[0]\n",
    "                rotary_class = rotary_class_dict[rotary_class_query]\n",
    "                if rotary_class == 'unbalance':\n",
    "                    severity = unbalance_severity_list[int(rotary_class_query[-1])-1]\n",
    "                elif rotary_class == 'misalignment':\n",
    "                    severity = misalignment_severity_list[int(rotary_class_query[-1])-1]\n",
    "                else:\n",
    "                    severity = 'none'\n",
    "                bearing_class_query = file_name.split('_')[1]\n",
    "                bearing_class = bearing_class_dict[bearing_class_query]\n",
    "                \n",
    "                class_name = rotary_class\n",
    "                folder_dist = os.path.join(new_snu_root, class_name)\n",
    "                if not os.path.exists(folder_dist):\n",
    "                    os.mkdir(folder_dist)\n",
    "                    \n",
    "                # Slice data_pd by 1-second window with 0.5-second stride\n",
    "                window_samples = int(window_size * sampling_rate)\n",
    "                stride_samples = int(stride * sampling_rate)\n",
    "                num_rows = len(disk_y)\n",
    "\n",
    "                for slice_start in range(0, num_rows - window_samples + 1, stride_samples):\n",
    "                    slice_end = slice_start + window_samples\n",
    "                    sliced_data = disk_y[slice_start:slice_end]\n",
    "                    \n",
    "                    # Save sliced data to a CSV file\n",
    "                    output_file = os.path.join(folder_dist, f'{rotary_class}+{bearing_class}_{sampling_rate_str}_{rpm}_{severity}_{load_condition}_{len(os.listdir(folder_dist))+1}.csv')\n",
    "                    data_pd = pd.DataFrame(sliced_data, columns=['disk_y'])\n",
    "                    \n",
    "                    data_pd.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/lilmae/Desktop/Rotray_Machine_Data/dataset/5_SNU/BearingType_CylindricalRoller/SamplingRate_8000/RotatingSpeed_600/H_B_8_N204_600.mat'\n",
    "mat_file = io.loadmat(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file['Data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
